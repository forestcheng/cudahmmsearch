% Chapter 5

\chapter{Benchmark results and discussion} % Main chapter title
\label{Results} % For referencing the chapter elsewhere, use \ref{Chapter1} 

\lhead{Chapter \ref{Results}. \emph{Results and discussion}} % This is for the header on each page - perhaps a shortened title

Chapter\ref{CUDAHMMER3} described several approaches to optimize the cudaSearch implementation. This chapter presents presents the performance measurements when experimenting these approaches on a GPU and on a multicore CPU.
%----------------------------------------------------------------------------------------

\section{Benchmarking environment}
The benchmarking environment were set up in Kronos machine \label{Kronos}as follows:

\begin{itemize}
 \item CPU host\\
 Intel® Core™ i7-3960X with 6 cores, 3.3GHz clock speed, 64GB RAM
 \item GPU device\\
 NVIDIA® Quadro® K4000 graphics card with 3 GB global memory, 768 Parallel-Processing Cores, 811 MHz GPU Clock rate, CUDA Capability version 3.0
 \item Software system\\
 The operating system used was Ubuntu 64 bit Linux v12.10; the CUDA toolkit used was version 5.5.
 \item Target sequences database\\
 One was Swiss-Prot fasta release September 2013 \citep{UniProt} with 540,958 sequences and another was much larger NCBI NR fasta release April 2014 \citep{NCBI} with 38,442,706 sequences.
 \item Query profile HMMs\\
 A profile HMM named globin4 with length of 149 states was distributed with the HMMER source \citep{Hsource}. 4 HMMs were taken directly from the Pfam database \citep{Pfam} that vary in length from 255 to 1111 states.
 \item Measuring method\\
 The execution time of the application was timed using the C clock() instruction. The performance was measured in unit GCUPS(Giga Cell Units Per Second) which is calculated as follows:
 \begin{equation*}
   GCUPS = \frac{L_q * L_t}{T * 1.0e09}
 \end{equation*}
 where $L_q$ is the length of query profile HMM, i.e. the number of the HMM states, $L_t$ is the total residues of target sequences in the database, $T$ is the execution time in second.\\
 All programs were compiled using GNU g++ with the -O3 option and executed independently in a 100\% idle system.
\end{itemize}

%----------------------------------------------------------------------------------------

\section{Performance Results}

\subsection{Comparison with less optimized approaches}
To show the performance impact of several selected optimization approaches, the performance of the implementation was compared with that of previous approach.

Table\ref{tab.opt} shows the approaches taken in optimizing performance. All tests are taken against the Swiss-Prot database. The query HMM used was globin4. The fourth column 'Improvement' is measured in percentage compared with the previous approach.

\begin{table}[H]
\centering
\begin{tabular}{|c|c|c|c|}\hline
\shortstack{\textbf{Description of} \\ \textbf{approach}} & \shortstack{\textbf{Execution} \\ \textbf{time (s)}} & \shortstack{\textbf{Performance}\\ (GCUPS)} & \shortstack{\textbf{Improvement}\\ \textbf{(\%)}}\\\hline
Initial implementation & 227.178 & 0.126 & - \\\hline
SIMD Video Instruction& 125.482 & 0.228 & 81 \\\hline
\shortstack{Minimizing global\\memory access} & 16.449 & 1.741 & 664 \\\hline
\shortstack{Async memcpy \&\\Multi streams} & 9.463 & 3.026 & 74 \\\hline
\shortstack{Coalescing of\\global memory\tablefootnote[51]{benchmarked only for $dp$ matrix}} & 6.565 & 4.362 & 44 \\\hline
Texture memory\tablefootnote[52]{benchmarked only for the query profile texOMrbv 2D texture} & 5.370 & 5.333 & 22 \\\hline
Sorting Database & 2.346 & 12.207 & 129 \\\hline
Distributing workload & 1.650 & 17.357 & 42 \\\hline
\end{tabular}
\caption{Performance of optimization approaches\label{tab.opt}}
\end{table}

The graphic view corresponding to Table is shown in Figure\ref{fig:imp}. 

\begin{figure}[!htb]
	\centering
	\includegraphics[totalheight=0.3\textheight]{Figures/improve.png}
	\caption{Performance of optimization approaches.}
	\label{fig:imp}
\end{figure}

From the chart, it can be seen that several factors are related to global memory accesses, including the highest 663\% minimizing global memory access, coalescing of global memory and texture memory. So the global memory optimizations are the most important area for performance. To make all threads in a warp execute similar tasks, the auxiliary sorting database also plays important role in optimizations.

\subsection{Practical benchmarks}
The final cudaSearch implementation, with the optimization discussed in Chapter\ref{CUDAHMMER3}, was benchmarked to determine its real-world performance. This was done by searching the much large NCBI NR database for several profile HMMs with various lengths, which were selected from Pfam database. As comparison, the same searches were executed by hmmsearch of HMMER3 on 1 CPU core.

The results of the benchmarks are shown in graphical form in Figure\ref{fig:len}. The GPU cudaSearch performance hovers just above 25 GCUPS, while the CPU hmmsearch only around 10 GCUPS. The whole performance of cudaSearch is stable with various lengths of query HMMs. On average, cudaSearch has a speedup of 2.5x than hmmsearch. The reason that the performance of both GPU and CPU searching for AAA\_27 dropped greatly will be explained in Section XXX.

\begin{figure}[!htb]
	\centering
	\includegraphics{Figures/lengths.png}
	\caption{Practical benchmarks.}
	\label{fig:len}
\end{figure}

\subsection{Comparison with multicore CPU}
Since multicore processors were developed in the early 2000s by Intel, AMD and others, nowadays CPU has become multicore with two cores, four cores, six cores and more. The Kronos \ref{Kronos} experiment system has CPU with six cores. This section presents the benchmarks of cudaSearch running with multiple CPU cores.

The experiment was done by executing cudaSearch and hmmsearch with 1, 2...6 CPU cores, searching the NCBI NR database for the HMM with 255-state length.

\begin{figure}[!htb]
	\centering
	\includegraphics{Figures/cpuCores.png}
	\caption{Comparison with multicore CPU.}
	\label{fig:cpuCores}
\end{figure}

Figure\ref{fig:cpuCores} shows the benchmark result. The number performance results is measured in GCUPS. As can be seen, from 1 CPU core to 4 CPU cores, both cudaSearch performance and hmmsearch performance go up almost linearly. From then on, due to complex schedule among CPU cores, the extra CPU core will not contribute much to both cudaSearch and hmmsearch execution. Even worse, it will have negative effect as shown clearly in the '6 CPU' case.

\subsection{Comparison with other implementations}
The performance of cudaSearch was also compared to the previous HMMER solutions: HMMER2.3.2 \citep{HMMER2}, GPU-HMMER2.3.2 \citep{GPUHMM} and HMMER3 \citep{Hsource}.

All tests are taken searching against the Swiss-Prot database for the globin4 profile HMM.

\begin{figure}[!htb]
	\centering
	\includegraphics{Figures/manyhmms.png}
	\caption{Comparison with other implementations.}
	\label{fig:hmms}
\end{figure}

As seen from the Figure\ref{fig:hmms}, since the release of HMMER2.3.2 in Oct 2003, accelerating hmmsearch researches on both CPU and GPU have achieved excellent improvement.
%----------------------------------------------------------------------------------------

\section{Speed benchmarking}

Compared to HMMER2, CUDA-HMMER2, HMMER3.1.

%----------------------------------------------------------------------------------------

% Chapter 6

\chapter{Conclusions and recommendations} % Main chapter title

\label{Conclusions} % For referencing the chapter elsewhere, use \ref{Chapter1} 

\lhead{Chapter 6. \emph{Conclusions and recommendations}} % This is for the header on each page - perhaps a shortened title

%----------------------------------------------------------------------------------------

\section{Performance}

%----------------------------------------------------------------------------------------

\section{Optimizing the implementation}


%----------------------------------------------------------------------------------------

\section{Recommendations for further research}

Multiple GPUs

Viterbi filter for no threshold

implementation for long seqence


%----------------------------------------------------------------------------------------
