
\chapter{Background} % Main chapter title

\label{Background} % For referencing the chapter elsewhere, use \ref{Background} 

\lhead{Chapter \ref{Background}. \emph{Background}} % This is for the header on each page - perhaps a shortened title

%----------------------------------------------------------------------------------------

\section{Sequence alignment and protein database}
\subsection{Cells, amino acids and proteins}

In 1665, Robert Hooke discovered the cell \citep{Cell}. The cell theory, first developed in 1839 by Matthias Jakob Schleiden and Theodor Schwann, generalized the view that \emph{all living organisms are composed of cells and of cell products} \citep{Loewy}. As workhorses of the cell, proteins not only constitute the major component in the cell, but they also regulate almost all activities that occurs in living cells.

Proteins are complex chains of small organic molecules known as \emph{amino acids}. In this thesis, \emph{residue} is used to refer to amino acids for protein. The 20 amino acids detailed in Table\ref{tab.amino} have been found within proteins and they convey a vast array of chemical versatility. So proteins can be viewed as sequences of an alphabet of the 20 amino acids \{A,C,D,E,F,G,H,I,K,L,M,N,P,Q,R,S,T,V,W,Y\}.

\begin{table}[H]
\centering
\begin{tabular}{|c|c|c|c|}\hline
\textbf{Letter} & \textbf{Amino acid} & \textbf{Letter} & \textbf{Amino acid} \\\hline
A & Alanine & C & Cysteine \\\hline
D & Aspartic acid & E & Glutamic acid \\\hline
F & Phenylalanine & G & Glycine \\\hline
H & Histidine & I & Isoleucine \\\hline
K & Lysine & L & Leucine \\\hline
M & Methionine & N & Asparagine \\\hline
P & Proline & Q & Glutamine \\\hline
R & Arginine & S & Serine  \\\hline
T & Threonine & V & Valine   \\\hline
W & Tryptophan & Y & Tyrosine  \\\hline
\end{tabular}
\caption{\fontfamily{pag}\selectfont The 20 amino acids\label{tab.amino}}
\end{table}

\subsection{Sequence alignment}
In bioinformatics, a sequence alignment is a way of arranging the sequences of protein to identify regions of similarity \citep{sa}. If two amino acid sequences are recognized as similar, there is a chance that they are \emph{homologous}. Homologous sequences share a common functional, structural, or evolutionary relationships between them. Protein sequences evolve with accumulating mutations. The basic mutational process are \emph{substitutions} where one residue is replaced by another, \emph{insertions} where a new residue is inserted into the sequence, and \emph{deletions}, the removal of a residue. Insertions and deletions are together referred to as \emph{gaps}. Table\ref{tab.sa} shows the sequence alignment of human beta globin (the \emph{query} sequence) and myoglobin (the \emph{target} sequence) and an internal gap is indicated by two dashes \citep{BioFunc}.

\begin{table}[H]
\centering
\begin{tabular}{|c|c|c|c|c|}\hline
\textbf{Query} & V T A L W & G K V N V & D -\hspace{6pt}-\hspace{6pt}E V & G G E A L \\\hline
\textbf{Target} & V L N V W & G K V E A & D I P G H & G Q E V L \\\hline
\emph{match} & 4\hspace{36pt} 11 & 6\hspace{8pt}5\hspace{8pt}4\hspace{19pt} & 6\hspace{44pt} & 6\hspace{18pt}5\hspace{18pt}4 \\\hline
\emph{mismatch} &-1-2 1 & \hspace{40pt}0\hspace{7pt}0 & \hspace{28pt}-2 -3 & -2\hspace{15pt} 0 \\\hline
\shortstack{\emph{gap open}\\ \emph{gap extend}} & & & \shortstack{-11\hspace{26pt} \\ -1} & \\\hline
\end{tabular}
\caption{\fontfamily{pag}\selectfont \textbf{Sequence alignment} \\sum of \emph{match}: +51\\sum of \emph{mismatch}: -9\\sum of gap penalties: -12\\total raw score: 51 - 9 -12 = 30\label{tab.sa}}
\end{table}

To establish the degree of homology, the two sequences are aligned: lined up in such a way that the degree of similarity also referred as score is maximized. Table\ref{tab.sa} illustrates how raw scores are calculated. The scores in the table for \emph{match/mismatch} are taken from the scoring matrix BLOSUM62 as shown in Figure\ref{fig:blo}. The BLOSUM62 matrix is a substitution matrix used for sequence alignment of proteins. It were first introduced in a paper by S. Henikoff and J.G. Henikoff \citep{Henikoff}. In a typical scoring scheme there are two gap penalties: one for \emph{gap open} (-11 in the example of Table\ref{tab.sa}) and one for \emph{gap extend} (-1 in Table\ref{tab.sa}).

\begin{figure}[!htb]
\centering
	\includegraphics[width=140mm]{Figures/BLOSUM62.png}
	\caption{\fontfamily{pag}\selectfont BLOSUM62 Scoring Matrix from \url{http://www.ncbi.nlm.nih.gov/Class/FieldGuide/BLOSUM62.txt}.}
	\label{fig:blo}
\end{figure}

Sorts of sequence alignment algorithms have been studied. Next chapter will introduce two sorts of algorithms: Smith-Waterman algorithm and HMM-based algorithms. They share a very general optimization technique called dynamic programming for finding optimal alignments.

\subsection{Bioinformatics protein databases}
This part is a brief introduction of two protein sequence databases used in this thesis.

\subsubsection*{NCBI NR databse}
The NCBI (National Center for Biotechnology Information) houses a series of databases relevant to Bioinformatics. Major databases include GenBank for DNA sequences and PubMed, a bibliographic database for the biomedical literature. Other databases include the NCBI Epigenomics database. All these databases are updated daily and available online: \url{http://www.ncbi.nlm.nih.gov/guide/all/\#databases\_}.

The NR (Non-Redundant) protein database maintained by NCBI as a target for their BLAST search services is a composite of SwissProt, SwissProt updates, PIR(Protein Information Resource), PDB(Protein Data Bank). Entries with absolutely identical sequences have been merged into NR database. The PIR produces the largest, most comprehensive, annotated protein sequence database in the public domain. The PDB is a repository for the three-dimensional structural data of large biological molecules, such as proteins and nucleic acids and is maintained by Brookhaven National Laboratory, USA.

Release 2014\_04 of NCBI NR databse contains 38,442,706 sequence entries, comprising 13,679,143,700 amino acids, more than 24GB in file size \citep{NCBI}.

\subsubsection*{Swiss-Prot}
The Universal Protein Resource (UniProt) is a comprehensive resource for protein sequence and annotation data and is mainly supported by the National Institutes of Health (NIH) \citep{upr}. The UniProt databases are the UniProt Knowledgebase (UniProtKB), the UniProt Reference Clusters (UniRef), and the UniProt Archive (UniParc). 

The UniProt Knowledgebase is updated every four weeks on average and consists of two sections: 
\begin{itemize}
 \item UniProtKB/Swiss-Prot\\
 This section contains manually-annotated records with information extracted from literature and curator-evaluated computational analysis. It is also highly cross-referenced to other databases. Release 2014\_05 of 14-May-2014 of UniProtKB/Swiss-Prot contains 545,388 sequence entries, comprising 193,948,795 amino acids abstracted from 228,536 references \citep{Swiss-Prot}.
 \item UniProtKB/TrEMBL\\
 This section contains computationally analyzed records that await full manual annotation. Release 2014\_05 of 14-May-2014 of UniProtKB/TrEMBL contains 56,010,222 sequence entries, comprising 17,785,675,050 amino acids \citep{UniProtTr}.
\end{itemize}

%----------------------------------------------------------------------------------------

\section{Dynamic programming in Bioinformatics}
Dynamic Programming (DP) is an optimization technique that recursively breaks down a problem into smaller subproblems, such that the solution to the larger problem can be obtained by piecing together the solutions to the subproblems \citep{BioMach}. This section shows how the Smith-Waterman algorithms and the algorithms in HMMER use DP for sequence alignment and database searches, and then discusses the related work about acceleration on CUDA-enabled GPU.

\subsection{The Smith-Waterman algorithm}
The Smith-Waterman algorithm is designed to find the optimal local alignment between two sequences. It was proposed by Smith and Waterman \citep{SW} and enhanced by Gotoh \citep{Gotoh}. The alignment of two sequences is based on dynamic programming approach by computing the similarity score which is given in the form of similarity score matrix $H$. 

Given a query sequence $Q$ with length $L_q$ and a target sequence $T$ with length $L_t$, let $S$ be the substitution matrix and its element $S[i,j]$ be the similarity score for the combination of the $i^{th}$ residue in $Q$ and the $j^{th}$ residue in $T$. Define $G_e$ as the gap extension penalty, and $G_o$ as the gap opening penalty. These similarity scores and $G_e$, $G_o$ are pre-determined by the life sciences community. The similarity score matrix $H$ for aligning $Q$ and $T$ is calculated as 

\begin{equation*}
   E[i, j] = max 
   \begin{cases}
    E[i, j-1]-G_e\\
    H[i, j-1]-G_o
   \end{cases}
\end{equation*}
\begin{equation*}
   F[i, j] = max
   \begin{cases}
    F[i-1, j]-G_e\\
    H[i-1, j]-G_o
   \end{cases}
\end{equation*}
\begin{equation*}
   H[i, j] = max
   \begin{cases}
    0\\
    E[i, j]\\
    F[i,j]\\
    H[i-1, j-1] + S[i,j]
   \end{cases}
\end{equation*}

where $1\leqslant i \leqslant L_q$ and $1\leqslant j \leqslant L_t$. The values for $E$, $F$ and $H$ are initialized as $E[i,0] = F[0,j] = H[i,0] = H[0,j]$ when $0\leqslant i \leqslant L_q$ and $0\leqslant j \leqslant L_t$.

The maximum value of the matrix $H$ gives the similarity score between $Q$ and $T$.

%----------------------------------------------------------------------------------------
\subsection{HMMER}

\label{HMMERsect}

HMMER \citep{HMMER} is a set of applications that create a profile Hidden Markov Model (HMM) of a sequence family which can be utilized as a query against a sequence database to identify (and/or align) additional homologs of the sequence family\citep{Seq}. HMMER was developed by Sean Eddy at Washington University and has become one of the most widely used software tools for sequence homology. The main elements of this HMM-based sequence alignment package are \emph{hmmsearch} and \emph{hmmscan}. The former searches for a profile HMM in a sequence database, while the latter searches for one or more sequences in profile HMMs database.

\subsubsection{HMM and profile HMM}

A hidden Markov model (HMM) is a computational structure for linearly analyzing sequences with a probabilistic method\citep{DicBioinfo}. HMMs have been widely used in speech signal, handwriting and gesture detection problems. In bioinformatics they have been used for applications such as sequence alignment, prediction of protein structure, analysis of chromosomal copy number changes, and gene-finding algorithm, etc\citep{BioFunc}. 

A HMM is a type of a non-deterministic finite state machine with transiting to another state and emitting a symbol under a probabilistic model.
According to \citep{SeqData}, a HMM can be defined as a 6-tuple ($A$, $Q$, $q_0$, $q_e$, $tr$, $e$) where \\[-1cm]

\begin{itemize}
\item \textbf{$A$} is a finite set (the alphabet) of symbols;
\item \textbf{$Q$} is a finite set of \emph{states};
\item \textbf{$q_0$} is the \emph{start} state and \textbf{$q_e$} is the \emph{end} state;
\item \textbf{$tr$} is the \emph{transition} mapping, which is the transition probabilities of state pairs in $Q$ $\times$ $Q$, satisfying the following two conditions: 
  \begin{enumerate}
   \item[(a)] $0 \leqslant tr(q,q') \leqslant 1$, $\forall q,q' \in Q$, and
   \item[(b)] for any given state $q$, such that:
   \begin{equation*}
    \displaystyle\sum_{q' \in Q}tr(q,q') = 1
   \end{equation*}
  \end{enumerate}
\item \textbf{$e$} is the \emph{emission} mapping, which is the emission probabilities of pairs in $Q$ $\times$ $A$, satisfying the following two conditions:
  \begin{enumerate}
   \item[(a)] $0 \leqslant e(q,x) \leqslant 1$, if it is defined, $\forall q \in Q$, and $x \in A$
   \item[(b)] for any given state $q$, if for any $x \in A$, $e(q, x)$ is defined, then $q$ is an \emph{emitting} state and
   \begin{equation*}
    \displaystyle\sum_{x \in A}e(q,x) = 1 
   \end{equation*}
   if $\forall x \in A$, $e(q, x)$ is not defined, then $q$ is a \emph{silent} state.
  \end{enumerate}
\end{itemize}

The dynamics of the system is based on Markov Chain, meaning that only the current state influences the selection of its successor ‚Äì the system has no `memory' of its history. Only the succession of characters emitted is visible; the state sequence that generated the characters remains internal to the system, i.e. hidden. By this means, the name is Hidden Markov Model\citep{IntroBio}. 

Profile HMM is a variant of HMM and can be constructed from an initial multiple sequence alignment to define a set of probabilities. The symbol sequence of an HMM is an observed sequence that resembles a consensus for the multiple sequence alignment. And a protein or gen family can be defined by profile HMMs.

In Figure\ref{fig:pHMM}, the internal structure of the ``Plan 7" profile HMM used by HMMER\citep{HMMER3} shows the mechanism for generating sequences. In order to generate sequences, a profile HMM should have a set of three states per alignment column: one \emph{match} state, one \emph{insert} state and one \emph{delete} state. 
\begin{itemize}
\item \textbf{\emph{Match states}} match and emit a amino acid from the query. The probability of emitting each of the 20 amino acids is a property of the model. 
\item \textbf{\emph{insert states}} allows the insert of one or more amino acids. The emission probability of this state is computed either from a background distribution of amino acids or from the observed insertions in the alignment.
\item \textbf{\emph{delete states}} skip the alignment column and emit a blank. Entering this state corresponds to gap opening, and the probabilities of these transitions reflect a position-specific gap penalty.
\end{itemize}

\begin{figure}[!htb]
	\includegraphics[width=150mm]{Figures/pHMM.png}
	\caption{\fontfamily{pag}\selectfont Profile HMM architecture used by HMMER\citep{HMMER3}.}
	\label{fig:pHMM}
\end{figure}

Begin at Start(S), and follow some chain of arrows until arriving at Termination(T). Each arrow transits to a state of the system. 
At each state, an action can be taken either as (1) emitting a residue, or (2) selecting an arrow to the next state. The action and the selection of successor state are governed by sets of probabilities\citep{IntroBio}.
The linear core model has five sets of match (M), insert (I) and delete (D) states. Each M state represents one consensus position and a set of M, I, D states is the main element of the model and is referred to as a ``node''ù in HMMER. Additional flanking states (marked as N, C, and J) emit zero or more residues from the background distribution, modelling nonhomologous regions preceding, following, or joining homologous regions aligned to the core model. Start (S), begin (B), end (E) and termination (T) states are non-emitting states.


A profile HMM for a protein family can be used to compare with target sequences, and classify sequences that are members of the family and those which are not\citep{ProteinBio}. 
A common application of profile HMMs is used to search a profile HMM against a sequence database. Another application is the query of a single protein sequence of interest against a database of profile HMMs.

\subsubsection{Viterbi algorithm in HMMER2}

\label{ViterbiSub}

In HMMER2, both \emph{hmmsearch} and \emph{hmmpfam} rely on the same core Viterbi algorithm for their scoring function which is named as \emph{P7Viterbi} in codes.

To find whether a sequence is member of the family described by a HMM, we compare the sequence with the HMM. We use an algorithm known as Viterbi to find one path that has the maximum probability of the HMM generating the sequence. Viterbi is a dynamic programming algorithm. Let $V_{i,j}$ be the maximum probability of a path from the start state $S_i$ ending at state $S_j$ and generating the prefix $q_{1...j}$ of the target sequence. $V_{i+1,j}$ is found by the recurrence:

\begin{equation*}
   \displaystyle V_{i+1,j} = \max_{0 \leqslant k \leqslant j-1} \big ( V_{i,k} P(k,j)P(q_{i+1} |j) \big )
\end{equation*}

% max |x| = 
%     \begin{cases}
%     \quad -x & \text{if } x < 0,\\
%     0 & \text{if } x = 0,\\
%     x & \text{if } x > 0.
%     \end{cases}
%     max = \left\{
% \begin{array}{rl}
% -x & \text{if } x < 0,\\
% 0 & \text{if } x = 0,\\
% x & \text{if } x > 0.
% \end{array} \right.

Define $a[i,j]$ as the transition probability from state $i$ to $j$ and $e_i$ as emission probability in state $i$.
Define $V_j^M(i)$ as the log-odds score of the optimal path matching subsequence $x_{1...i}$ to the submodel up to state $j$, ending with $x_i$ being emitted by \emph{match} state $M_j$. Similarly $V_j^I(i)$ is the score of the optimal path ending in $x_i$ being emitted by \emph{insert} state $I_j$, and $V_j^D(i)$ for the optimal path ending in \emph{delete} state $D_j$. $q_{x_i}$ is the probability of $x_i$. Then we can write the Viterbi general equation\citep{BioSeq}:

\begin{equation*}
   V_j^M(i) = \log\frac{e_{M_j}(x_i)}{q_{x_i}} + max 
   \begin{cases}
   V_{j-1}^M(i-1) + \log a[M_{j-1},M_j]\\
   V_{j-1}^I(i-1) + \log a[I_{j-1},M_j]\\
   V_{j-1}^D(i-1) + \log a[D_{j-1},M_j]
   \end{cases}
\end{equation*}

\begin{equation*}
   V_j^I(i) = \log\frac{e_{I_j}(x_i)}{q_{x_i}} + max 
   \begin{cases}
   V_j^M(i-1) + \log a[M_j,I_j]\\
   V_j^I(i-1) + \log a[I_j,I_j]
   \end{cases} 
\end{equation*}

\begin{equation*}
   V_j^D(i) = max 
   \begin{cases}
   V_{j-1}^M(i) + \log a[M_{j-1},D_j]\\
   V_{j-1}^D(i) + \log a[D_{j-1},D_j]
   \end{cases}  
\end{equation*}

Based on the above equations, we can write the efficient pseudo code of Viterbi algorithm, as shown in Algorithm\ref{Viterbi} \citep{FPGA}.  The inner loop of the code contains three two dimensional matrices (M, I, D), which calculate scores of all node positions involved in the main models for each of the residue. The outer loop consists of flanking and special states calculated in the one dimensional arrays N, B, C, J, E.

\renewcommand{\thepseudonum}{\roman{pseudonum}}
\begin{pseudocode}{Viterbi}{ }
\label{Viterbi}
\COMMENT{Initialization}\\
N[0] \GETS 0; \ \  B[0] \GETS tr(N, B)\\
E[0] \GETS C[0] \GETS J[0] \GETS -\infty\\
\COMMENT{for every sequence residue i}\\
\FOR i \GETS 1 \TO L_t \DO
\BEGIN
  N[i] \GETS N[i-1] + tr(N, N)\\
  B[i] \GETS max 
  \begin{cases}
   N[i-1] + tr(N, B)\\
   J[i-1] + tr(J, B)
  \end{cases}\\
  M[i,0] \GETS I[i,0] \GETS D[i,0] \GETS -\infty\\
  \COMMENT{For every model position j from 1 to $L_q$}\\
  \FOR j \GETS 1 \TO L_q \DO
  \BEGIN
    M[0, j] \GETS I[0, j] \GETS D[0, j] \GETS -\infty\\
    M[i, j] \GETS e(M_j, S[i]) + max 
    \begin{cases}
     M[i-1, j-1] + tr(M_{j-1}, M_j)\\
     I[i-1, j-1] + tr(I_{j-1}, M_j)\\
     D[i-1, j-1] + tr(D_{j-1}, M_j)\\
     B[i-1] + tr(B, M_j)
    \end{cases}\\
    I[i, j] \GETS e(I_j, S[i]) + max
    \begin{cases}
     M[i-1, j] + tr(M_j, I_j)\\
     I[i-1, j] + tr(I_j, I_j)
    \end{cases}\\
    D[i, j] \GETS max
    \begin{cases}
     M[i, j-1] + tr(M_{j-1}, D_j)\\
     D[i, j-1] + tr(D_{j-1}, D_j)
    \end{cases}\\
  \END\\
  E[i] \GETS max\{M[i,j] + tr(M_j, E)\} \ \  (j \GETS 0 \TO L_q)\\
  J[i] \GETS max 
  \begin{cases}
   J[i-1] + tr(J, J)\\
   E[i-1] + tr(E, J)
  \end{cases}\\
  C[i] \GETS max
  \begin{cases}
   C[i-1] + tr(C, C)\\
   E[i-1] + tr(E, C)
  \end{cases}\\
\END\\
\COMMENT{Termination: }\\
\RETURN {T(S,M) \GETS C[L_t] + tr(C,T)}
\end{pseudocode}

From Algorithm\ref{Viterbi}, we can see the fundamental task of the Viterbi algorithm for Biological Sequence Alignment is to calculate three DP(Dynamic Programming) matrices: $M[{ }]$ for Match state, $I[{ }]$ for Insert state and $D[{ }]$ for Delete state. Each DP matrix consisting of $(L_t+1) * (L_q+1)$ blocks, where each value of block is dependent on the value of previous block. As shown in Figure\ref{fig:dpV}, the Match state $M[i,j]$ depends on the upper-left block $M[i-1, j-1]$, $I[i-1, j-1]$ and $D[i-1, j-1]$; the Insert state $I[i,j]$ depends on the left block $M[i-1, j]$ and $I[i-1, j]$; and the Delete state depends on the upper block $M[i, j-1]$ and $D[i, j-1]$.

\begin{figure}[!htb]
\centering
	\includegraphics[width=120mm]{Figures/dpViterbi.png}
	\caption{\fontfamily{pag}\selectfont \textbf{The DP matrix calculated in Viterbi algorithm.} 
The rectangle on the left represents the whole matrix to be calculated by the Viterbi algorithm, and the right rectangle of the figure shows the process of updating a single block of the matrix.}
	\label{fig:dpV}
\end{figure}

\subsubsection{MSV algorithm in HMMER3}

\label{MSVsub}

HMMER3 is near rewrite of the earlier HMMER2 package, with the aim of improving the speed of profile HMM searches. The main performance gain is due to a heuristic algorithm called MSV filter, for Multiple (local, ungapped) Segment Viterbi. MSV is implemented in SIMD(Single-Instruction Multiple-Data) vector parallelization instructions and is about 100-fold faster than HMMER2.

\begin{figure}[!htb]
	\includegraphics[width=150mm]{Figures/pHMM_msv.png}
	\caption{\fontfamily{pag}\selectfont MSV profile: multiple ungapped local alignment segments \citep{HMMER3}.}
	\label{fig:pMSV}
\end{figure}

Figure\ref{fig:pMSV} illustrates the MSV profile architecture. Compared with Figure\ref{fig:pHMM}, the MSV corresponds to the virtual removal of the delete and insert states. All match-match transition probabilities are treated as 1.0. The rest parameters remains unchanged. So this model generates sequences containing one or more ungapped local alignment segments. The pseudo code of MSV score algorithm is simplified and shown in Algorithm\ref{MSV}.

Figure\ref{fig:dpMSV} illustrates an example of an alignment of a MSV profile HMM model (length $L_q = 14$) to a target sequence (length $L_t=22$). A path to generate the target sequence with the profile HMM model is shown through a dynamic programming (DP) matrix. The model identifies two high-scoring ungapped alignment segments, as shown in black dots, indicating residues aligned to profile match states. All other residues are assigned to N, J, and C states in the model, as shown in orange dots. Unfilled dot indicates a ``mute'' non-emitting state or state transition.

\begin{figure}[!htb]
\centering
	\includegraphics[width=100mm]{Figures/dpMSV.png}
	\caption{\fontfamily{pag}\selectfont \textbf{Example of an MSV path in DP matrix \citep{HMMER3}.} 
An alignment of a MSV profile HMM model (length $L_q = 14$) to a target sequence (length $L_t=22$). A path from top to bottom is through a dynamic programming (DP) matrix. The model identifies two high-scoring ungapped alignment segments, as shown in black dots, indicating residues aligned to profile match states. All other residues are assigned to N, J, and C states in the model, as shown in orange dots. Unfilled dot indicates a ``mute'' non-emitting state or state transition.}
	\label{fig:dpMSV}
\end{figure}

\begin{pseudocode}{MSV}{ }
\label{MSV}
\COMMENT{Initialization}\\
N[0] \GETS 0; \ \  B[0] \GETS tr(N, B)\\
E[0] \GETS C[0] \GETS J[0] \GETS -\infty\\
\COMMENT{for every sequence residue i}\\
\FOR i \GETS 1 \TO L_t \DO
\BEGIN
  N[i] \GETS N[i-1] + tr(N, N)\\
  B[i] \GETS max 
  \begin{cases}
   N[i-1] + tr(N, B)\\
   J[i-1] + tr(J, B)
  \end{cases}\\
  M[i,0] \GETS -\infty\\
  \COMMENT{For every model position j from 1 to $L_q$}\\
  \FOR j \GETS 1 \TO L_q \DO
  \BEGIN
    M[0, j] \GETS -\infty\\
    M[i, j] \GETS e(M_j, S[i]) + max 
    \begin{cases}
     M[i-1, j-1]\\
     B[i-1] + tr(B, M_j)
    \end{cases}\\
  \END\\
  E[i] \GETS max\{M[i,j] + tr(M_j, E)\} \ \  (j \GETS 0 \TO L_q)\\
  J[i] \GETS max 
  \begin{cases}
   J[i-1] + tr(J, J)\\
   E[i-1] + tr(E, J)
  \end{cases}\\
  C[i] \GETS max
  \begin{cases}
   C[i-1] + tr(C, C)\\
   E[i-1] + tr(E, C)
  \end{cases}\\
\END\\
\COMMENT{Termination: }\\
\RETURN {T(S,M) \GETS C[L_t] + tr(C,T)}
\end{pseudocode}


\textbf{SIMD vector parallelization in HMMER3}
\label{SSE2}

Single-Instruction Multiple-Data (SIMD) instruction is able to perform the same operation on multiple pieces of data in parallel. The SIMD vector instruction sets use 128-bit vector registers to compute up to 16 simultaneous operations. Due to the huge number of iterations in the Smith-Waterman algorithm calculation, using SIMD instructions to reduce the number of instructions needed to perform one cell calculation has a significant impact on the execution time. Several SIMD vector parallelization methods have been described for accelerating SW dynamic programming. 

In 2000, Rognes and Seeberg presented an implementation of the SW algorithm running on the Intel Pentium processor using the MMX SIMD instructions \citep{SW-SIMD}. They used a query profile parallel to the query sequence for each possible residue. A query profile was pre-calculated in a sequential layout just once before searching database. A six-fold speedup was reported over an optimized non-SIMD implementation. 

In 2007, Farrar presented an efficient vector-parallel approach called striped layout for vectorizing SW algorithm \citep{SW-SSE2}. He designed a striped query profile for SIMD vector computation. He used Intel Streaming SIMD Extensions 2 (SSE2) to implement his design. A speedup of 2‚Äì8 times was reported over the Rognes and Seeberg SIMD non-stripped implementations.

Similarly, since the MSV model removes \emph{delete} and \emph{insert} states that interfere with vector parallelism, the striped vector-parallel technique can be remarkably efficient way to calculate MSV alignment scores. To maximize parallelism, in HMMER3\citep{HMMER3}, Sean R. Eddy implemented MSV as a 16-fold parallel calculation with score values stored as 8-bit byte integers. He used SSE2 instructions on Intel-compatible systems and Altivec/VMX instructions on PowerPC systems.

Figure\ref{fig:strip} shows the stripped pattern. The query profile HMM of length $L_q$ is divided into vectors with equal length $L_v$. The vector length $L_v$ is equal to the number of elements being processed in the 128-bit SIMD register. MSV processes 8-bit byte integer with $L_v$ = 128/8 = 16. In a row-vectorized implementation, the query profile HMM is stored in the vectorized dynamic programming matrix dp. The dp stores $L_q$ cells in $L_Q$ vectors which is numbered as $q = 1...L_Q$, where $L_Q = (L_q+L_v-1)/L_v$. Figure\ref{fig:strip} illustrates $L_q$ cells assigned to $L_Q$ vectors in a non-sequential way. For simple illustration, $L_v = 4, L_q = 14$, and $L_Q = 4$.

\begin{figure}[!htb]
	\includegraphics{Figures/msv_vector.jpg}
	\caption{\fontfamily{pag}\selectfont Illustration of striped indexing for SIMD vector calculations\citep{HMMER3}.}
	\label{fig:strip}
\end{figure}

In Smith-Waterman and Viterbi dynamic programming, the calculation of each cell $(i, j)$ in the dp is dependent on previously calculated cells $(i-1, j), (i, j-1)$ and $(i-1, j-1)$. However, in MSV algorithm, the \emph{delete} and \emph{insert} states have been removed and only ungapped diagonals need calculating, so the calculation of each cell $(i, j)$ requires only previous $(i-1, j-1)$. In Figure\ref{fig:strip}, the top red row shows the previous row $i-1$ for the cells $j-1$, which is needed for calculating each new cell $j$ in a new row $i$. 

Striping method can remove the SIMD register data dependencies. As can be seen in the Figure\ref{fig:strip}, with striped indexing, vector $q-1$ contains exactly the four $j-1$ cells needed to calculate the four cells $j$ in a new vector $q$ on a new blue row of the dp matrix. For example, when we calculate cells $j=(2,6,10,14)$ in vector $q=2$, we access the previous row‚Äôs vector $q-1=1$ which contains the cells we need in the order we need them, $j-1=(1,5,9,13)$ (the vector above). 

\begin{figure}[!htb]
\centering
	\includegraphics[width=110mm]{Figures/msv_nostrip.jpg}
	\caption{\fontfamily{pag}\selectfont Illustration of linear indexing for SIMD vector calculations.}
	\label{fig:nostrip}
\end{figure}

Instead, if we indexed cells into vectors in the linear order ($j=1,2,3,4$ in vector $q=1$ and so on), as shown in Figure\ref{fig:nostrip}, there is no such correspondence of $(q,q-1)$ with four $(j-1,j)$, and each calculation of a new vector $q$ would require extra expensive operations, such as shifting or rearranging cell values inside the previous row's vectors. By using the striped query access, only one shift operation is needed per row as shown in \ref{fig:strip}. Outside the inner loop(for $q = 1$ to $L_Q$), the last vector on each finished row is right-shifted (mpv, in grey with red cell $j$ indices) and used to initialize the next row calculation.

The pseudo code for the implementation is shown in Algorithm\ref{MSV-SIMD}

\begin{pseudocode}{MSV-SIMD}{ }
\label{MSV-SIMD}
\COMMENT{Initialization}\\
xJ \GETS 0; \ \  dp[q] \GETS vec\_splat(0) \  (q \GETS 0 \TO L_Q-1)\\
xB \GETS base + tr(N, B)\\
xBv \GETS vec\_adds(xB, tr(B, M))\\
\COMMENT{for every sequence residue i}\\
\FOR i \GETS 1 \TO L_t \DO
\BEGIN
  xEv \GETS vec\_splat(0)\\
  mpv \GETS vec\_rightshift(dp[L_Q-1])\\
  \FOR q \GETS 0 \TO L_Q-1 \DO
  \BEGIN
    \COMMENT{temporary  storage of 1 current row value in progress}\\
    tmpv \GETS vec\_max(mpv, xBv)\\
    tmpv \GETS vec\_adds(tmpv, e(M_j, S[i]))\\
    xEv \GETS vec\_max(xEv, tmpv)\\
    mpv \GETS dp[q]\\
    dp[q] \GETS tmpv\\
  \END\\
  xE \GETS vec\_hmax(xEv)\\
  xJ \GETS max 
  \begin{cases}
   xJ\\
   xE + tr(E, J)
  \end{cases}\\
  xB \GETS max 
  \begin{cases}
   base\\
   xJ + tr(J, B)
  \end{cases}\\
\END\\
\COMMENT{Termination: }\\
\RETURN {T(S,M) \GETS xJ + tr(C,T)}
\end{pseudocode}

Five pseudocode vector instructions for operations on 8-bit integers are used in the pseudo code. Either scalars $x$ or vectors v containing 16 8-bit integer elements numbered $v[0]...v[15]$. Each of these operations are either available or easily constructed in Intel SSE2 intrinsics as shown in the following table.

% \begin{minipage}{\textwidth}
% \begin{center}
% \begin{tabular}{|c|c|c|}\hline
% \shortstack{\textbf{Pseudocode} \\ SSE2 code in C} & \textbf{Operation} & \textbf{Definition}\\\hline
% \shortstack{\textbf{v = vec\_splat(x)} \\ v = \_mm\_set1\_epi8(x)} & assignment & $v[z] = x$\\\hline
% \shortstack{\textbf{v = vec\_adds(v1, v2)} \\ v = \_mm\_adds\_epu8(v1, v2)} & saturated addition & $v[z] = min$
% $\begin{cases}
%   2^8-1\\
%   v1[z]+v2[z]
% \end{cases}$\\\hline
% \shortstack{\textbf{v1 = vec\_rightshift(v2)} \\ v1 = \_mm\_slli\_si128(v2, 1)} & right shift & \shortstack{$v1[z] = v2[z-1](z=16...1)$; \\ $v1[0]=0;$}\\\hline
% \shortstack{\textbf{v = vec\_max(v1, v2)} \\ v = \_mm\_max\_epu8(v1, v2)} & max & $v[z] = max(v1[z], v2[z])$\\\hline
% \shortstack{\textbf{x = vec\_hmax(v)} \\ -} & horizontal max & $x = max_zv[z]$\\\hline
% \end{tabular}
% \end{center}
% \end{minipage}


\begin{table}[H]
\centering
\begin{tabular}{|c|c|c|}\hline
\shortstack{\textbf{Pseudocode} \\ SSE2 intrinsic in C} & \textbf{Operation} & \textbf{Definition}\\\hline
\shortstack{\textbf{v = vec\_splat(x)} \\ v = \_mm\_set1\_epi8(x)} & assignment & $v[z] = x$\\\hline
\shortstack{\textbf{v = vec\_adds(v1, v2)} \\ v = \_mm\_adds\_epu8(v1, v2)} & saturated addition & $v[z] = min$
$\begin{cases}
  2^8-1\\
  v1[z]+v2[z]
\end{cases}$\\\hline
\shortstack{\textbf{v1 = vec\_rightshift(v2)} \\ v1 = \_mm\_slli\_si128(v2, 1)} & right shift & \shortstack{$v1[z] = v2[z-1](z=15...1)$; \\ $v1[0]=0;$}\\\hline
\shortstack{\textbf{v = vec\_max(v1, v2)} \\ v = \_mm\_max\_epu8(v1, v2)} & max & $v[z] = max(v1[z], v2[z])$\\\hline
\shortstack{\textbf{x = vec\_hmax(v)} \\ -} & horizontal max & $x = max(v[z]),z=0...15$\\\hline
\end{tabular}
\caption{\fontfamily{pag}\selectfont\textbf{SSE2 intrinsics for pseudocode in Algorithm\ref{MSV-SIMD}} The first column is pseudocode and its corresponding SSE2 intrinsic in C language. Because x86 and x86-64 use little endian, \textbf{vec\_rightshift()} means using a left bit shift intrinsic \textbf{\_mm\_slli\_si128()} to do right shift. No SSE2 intrinsic is corresponding to \textbf{vec\_hmax()}. Shuffle intrinsic \textbf{\_mm\_shuffle\_epi32} and \textbf{\_mm\_max\_epu8} can be combined to implement \textbf{vec\_hmax()}.\label{tab.SSE2}}
\end{table}


%----------------------------------------------------------------------------------------

\section{CUDA accelerated sequence alignment}
\label{CUDASeqAlign}

In November 2006, NVIDIA\textregistered introduced CUDA\texttrademark (Compute Unified Device Architecture), a general purpose parallel computing platform and programming model that enables users to write scalable multi-threaded programs in NVIDIA GPUs. Nowadays there exist alternatives to CUDA, such as OpenCL \citep{OpenCL}, Microsoft Compute Shader\citep{Shader}. These are mostly similar, but as CUDA is the most widely used and more mature, this thesis will focus on that.

This section firstly overviews CUDA programming model, then reviews recent studies on accelerating Smith-waterman algorithm and HMM-based algorithms on CUDA-enabled GPU.

\subsection{Overview of CUDA programming model}
\subsubsection{Streaming Multiprocessors}
A GPU consists of one or more SMs(Streaming Multiprocessors). Quadro K4000 used in our research has 4 SMs. Each SM contains the following specific features \citep{CUDAHand}:

\begin{itemize}
 \item Execution units to perform integer and single- or double-precision floating-point arithmetic, Special function units (SFUs) to compute single-precision floating-point transcendental functions
 \item Thousands of registers to be partitioned among threads
 \item Shared memory for fast data interchange between threads
 \item Several caches, including constant cache, texture cache and L1 cache
 \item A warp scheduler to coordinate instruction dispatch to the execution units
\end{itemize}

The SM has been evolving rapidly since the introduction of the first CUDA-enabled GPU device in 2006, with three major Compute Capability 1.x, 2.x, and 3.x, corresponding to Tesla-class, Fermi-class, and Kepler-class hardware respectively. Table\ref{tab.sm} summarizes the features introduced in each generation of the SM hardware \citep{CUDAHand}.

\begin{table}[H]
\begin{tabular}[t]{|c|c|}\hline
\shortstack{\textbf{Compute}\\ \textbf{Capability} } & \textbf{Features introduced} \\\hline
SM 1.x & \shortstack[l]{Global memory atomics; mapped pinned memory; debuggable;\\ atomic operations on shared memory; Double precision} \\\hline
{SM 2.x} & \shortstack[l]{64-bit addressing; L1 and L2 cache; concurrent kernel execution;\\ global atomic add for single-precision floating-point values;\\ Function calls and indirect calls in kernels} \\\hline
SM 3.x & \shortstack[l]{SIMD Video Instructions; Increase maximum grid size; warp shuffle;\\ Bindless textures (``texture objects''); read global memory via texture;\\ faster global atomics; 64-bit atomic min, max, AND, OR, and XOR;\\ dynamic parallelism} \\\hline
\end{tabular}
\caption{\fontfamily{pag}\selectfont Features per Compute Capability\label{tab.sm}}
\end{table}

\subsubsection{CUDA thread hierarchy}
The execution of a typical CUDA program is illustrated in Figure\ref{fig:exeCUDA} The CPU host invokes a GPU kernel in-line with the triple angle-bracket $<<<$  $>>>$ syntax from CUDA C/C++ extension code. The kernel is executed N times in parallel by N different CUDA threads. All the threads that are generated by a kernel during an invocation are collectively called a \emph{grid}. Figure\ref{fig:exeCUDA} shows the execution of two grids of threads.

\begin{figure}[!htb]
	\centering
	\includegraphics[totalheight=0.2\textheight]{Figures/exeCUDA.png}
	\caption{\fontfamily{pag}\selectfont Execution of a CUDA program\citep{Kirk}.}
	\label{fig:exeCUDA}
\end{figure}

Threads in a grid are organized into a two-level hierarchy, as illustrated in Figure\ref{fig:grid}. At the top level, each grid consists of one or more thread blocks. All blocks in a grid have the same number of threads and are organized into a one, two, or three-dimensional \emph{grid} of thread blocks.

\begin{figure}[!htb]
	\centering
	\includegraphics[totalheight=0.4\textheight]{Figures/gridBlock.png}
	\caption{\fontfamily{pag}\selectfont CUDA thread organization\citep{Zeller}.}
	\label{fig:grid}
\end{figure}

Each block can be identified by an index accessible within the kernel through the built-in \emph{blockIdx} variable. The dimension of the thread block is accessible within the kernel through the built-in \emph{blockDim} variable.

The threads in a block are executed by the same multiprocessor within a GPU. They can cooperate by sharing data through some shared memory and by synchronizing their execution to coordinate memory accesses. Each block can be scheduled on any of the available multiprocessors, in any order, concurrently or sequentially, so that a compiled CUDA program can execute on any number of multiprocessors. On the hardware level, a block's threads are executed in parallel as \emph{warps} which name originate from \emph{weaving loom}. A warp consists of 32 threads.

\subsubsection{CUDA memory hierarchy}
Besides the threading model, another thing that makes CUDA programming different from a general purpose CPU is its memory spaces, including registers, local, shared, global, constant and texture, as shown in Figure\ref{fig:cudaMem}

\begin{figure}[!htb]
	\centering
	\includegraphics[totalheight=0.4\textheight]{Figures/cudaMem.png}
	\caption{\fontfamily{pag}\selectfont CUDA memory organization\citep{Zeller}.}
	\label{fig:cudaMem}
\end{figure}

CUDA memory spaces have different characteristics that reflect their distinct usages in CUDA applications as summarized in Table\ref{tab.mem} \citep{CUDABest}.

\begin{table}[H]
\centering
\begin{tabular}{|c|c|c|c|c|c|c|}\hline
\textbf{Memory} & \textbf{Location} & \textbf{Cached} & \textbf{Access} & \textbf{Scope} & \textbf{Speed} & \textbf{Lifetime} \\\hline
Register & On chip & n/a & R/W & 1 Thread & 1 & Thread \\\hline
Local & Off chip & \dag & R/W & 1 Thread & $\sim 2 - 16$ & Thread \\\hline
Shared & On chip & n/a & R/W & \shortstack{All threads\\ in block} & $\sim 2 - 16$ & Block \\\hline
Global & Off chip & \dag & R/W & \shortstack{All threads\\ + host} & 200+ & Host allocation \\\hline
Constant & Off chip & Yes & R & \shortstack{All threads\\ + host} & $2 - 200$ & Host allocation \\\hline
Texture & Off chip & Yes & R & \shortstack{All threads\\ + host} & $2 - 200$ & Host allocation \\\hline
\end{tabular}
\caption{\fontfamily{pag}\selectfont \textbf{Salient Features of GPU Device Memory.} \textbf{Speed} column is the relative speed in number of instructions. {\dag} means it is cached only on devices of above compute capability 2.x. \label{tab.mem}}
\end{table}

\subsubsection{CUDA tools}
% http://docs.nvidia.com/cuda/profiler-users-guide/#axzz32CP5hrTb
%----------------------------------------------------------------------------------------

\subsection{CUDA accelerated Smith-Waterman}
The Smith-Waterman algorithm for sequence alignment uses dynamic programming method for sequence alignment, which is also the characteristic of HMM-based algorithms. In this section, we review the techniques used in parallelizing Smith-Waterman on a CUDA-enabled GPU and these techniques will be evaluated for accelerating MSV algorithm in Chapter \ref{CUDAHMMER3}.

\subsubsection*{Parallelism strategy applied}
\citep{Manavski}, \citep{SW++}, \citep{Akoglu}, \citep{Ligowski}, \citep{SW++2}, \citep{Kentie}, \citep{SW++3} used task-based parallelism to process each target sequence independently with a single GPU thread.

\citep{SW++} used data-based parallelism to support longest query/target sequences.

\citep{Saeed} formulated parallel version of the Smith-Waterman algorithm and used MPI over 100 Mb Ethernet to extend work to multiple GPUs.

\subsubsection*{Processing target sequences database}
\citep{Manavski}, \citep{Akoglu}, \citep{SW++3} presorted database in ascending order.

\citep{Ligowski} presorted database in descending order and organized in blocks consisting of 256 sequences.\citep{Kentie} presorted database also in descending order and converted into a special format. 

\subsubsection*{Device memory access pattern}
\citep{SW++} sorted target sequences and arranged in an array like a multi-layer bookcase to store into global memory, so that the reading of the database across multiple threads could be coalesced. Writes to global memory were first batched in shared memory for better coalescing.  Due to a reduction in the global memory accesses, they proposed a cell block division method for the task-based parallelization, where the alignment matrix is divided into cell blocks of equal size.

\citep{Kentie} reduced global memory access by making temporary values interleaved and read/wrote score and Ix temporary values in one access.

\citep{SW++} exploited constant memory to store the gap penalties, scoring matrix and the query sequence. And \citep{Kentie} stored gap penalties in constant memory. \citep{Akoglu} mapped query sequence as well as the substitution matrix to the constant memory.

\citep{Manavski}, \citep{SW++2} used texture memory to store query profiles. And \citep{Kentie} used texture for substitution matrix.

\citep{SW++} loaded the scoring matrix into shared memory.

\citep{Ligowski} reduce global memory access only at the loop initialization and for writing the results at the exit. They performed all operations within the loop in fast shared memory and registers.

\subsubsection*{Vector programming model}
\citep{Manavski} packaged the query profile in texture memory, storing 4 successive values into the 4-byte of a single unsigned integer. And they read at a time 4 H and 4 E values from local memory.

\citep{Akoglu} calculated the Smith-Waterman score from the query sequence and database sequences by means of columns, four cells at a time.

\citep{SW++2} designed a striped query profile for SIMD vector computation and used a packed data format to store into the CUDA built-in \emph{uchar4} vector data type. They divided a query sequence into a series of non-overlapping, consecutive small partitions with a specified partition length, and then aligned the query sequence to a subject sequence partition by partition. They ported the SIMD CPU algorithm \citep{SW-SSE2} to the GPU, viewing collections of processing elements as part of a single vector.

\citep{Kentie} loaded 4 query characters at a time and processed 8 database characters at a time.

\citep{SW++3} designed a query profile variant data structure and used the built-in \emph{uint4} vector data type to store each sequence profile for quad-lane SIMD computing on GPUs. They used CUDA SIMD Video Instructions in GPU computing and used Intel SSE2 intrinsics in CPU computing.

\subsubsection*{Miscellaneous techniques}
\citep{Manavski} pre-computed a query profile parallel to the query sequence for each possible residue and achieved dynamic load balancing between multiple GPUs according to their computational power at run time.

\citep{Kentie} simplified substitution matrix lookup by using numeric values instead of letters for sequence symbols.

\citep{SW++3} distributed workload between CPU and GPU.

%----------------------------------------------------------------------------------------

\subsection{CUDA accelerated HMMER}
HMMER includes a MPI (Message Passing Interface) implementation of the searching algorithms, which uses conventional CPU clusters for parallel computing. Since \citep{ClawHMMER} \footnote{ClawHMMer is based on the BrookGPU stream programming language, not CUDA programming model.}, the first GPU-enabled \emph{hmmsearch} implementation, there has been several researches on accelerating HMMER for CUDA-enabled GPU. The following is the summary of techniques applied by 5 research work.

\subsubsection*{Parallelism strategy applied}
\citep{GPUHMM}, \citep{Quirem} and \citep{Ahmed} used task-based parallelism to process each target sequence independently with a single GPU thread.

\citep{Du} used data-based parallelism by dividing the basic computational kernel of the Viterbi algorithm of each tile into two parts, independent and dependent parts.

\citep{Ganesan} presented a hybrid parallelization strategy by combining task-based and data-based parallelism. They parallelized evaluations of recurrence equations by partitioning the chain of dependencies in a uniform and regular fashion.

\subsubsection*{HMM-based algorithm researched}
\citep{GPUHMM}, \citep{Ganesan}, \citep{Du} and \citep{Quirem} parallelized Viterbi algorithm.

\citep{Ahmed} used IntelÆ VTune Analyzer \citep{Intel} to investigate performance hotspot functions in HMMER3. Based on hotspot analysis, they studied CUDA acceleration for three individual algorithm: Forward, Backward and Viterbi algorithms. And they also found data transfer overhead between heterogeneous processors could be a performance bottleneck.

However, our research focus on the MSV algorithm in HMMER3.

\subsubsection*{Device memory access pattern}
\citep{GPUHMM} made use of high speed texture memory to store both the target sequence batch as well as the query profile HMM. Their benchmark result showed that global memory coalescing contributed an improvement of more than 9x for larger HMMs. They also used constant memory to store the HMM and used shared memory to temporarily store the index into each threadís digitized sequence.

\citep{Ganesan} adopted the partitioning scheme to coalesce memory access by storing lookup data of model positions at regular intervals
contiguously.

\citep{Du} reorganized the computational kernel of the Viterbi algorithm, and divided the basic computing unit into two parts: independent and dependent parts. All of the independent parts are executed with a parallel and balanced load in an optimized coalesced global memory access manner, which significantly improves the Viterbi algorithmís performance on GPU. 

\citep{Quirem} utilized pinned memory to reduce the latency induced by transferring memory from device to host and back.

\subsubsection*{Miscellaneous techniques}
\begin{itemize}
 \item \citep{GPUHMM} created two CPU threads for reading database and post-processing the database hits.
 \item \citep{GPUHMM} presorted target sequences database in ascending order.
 \item \citep{GPUHMM} applied loop unrolling which is a classic loop optimization strategy designed to reduce the overhead of inefficient looping.
\end{itemize}

